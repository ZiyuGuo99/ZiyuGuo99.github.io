<head>
  <style>
    .publication-title {
      color: blue;
    }
    .publication-authors {
      color: black;
    }
    .publication-periodical {
      color: purple;
    }
    .pub-section {
      font-size: 18px;
      font-weight: 700;
      color: #4a148c;           
      background: linear-gradient(90deg, #e8def8 0%, #ffffff 80%);
      padding: 6px 12px;
      margin: 30px 0 10px 0;
      border-radius: 6px;
      box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
  </style>
</head>
<h2 id="publications" style="margin: 2px 0px -15px;">Selected Publications and Preprints</h2>

<div class="publications">

  
<strong><i style="color:#7b5aa6">* Equal contribution, # Corresponding author</i></strong>
<div class="pub-section">ðŸ§  Multi-modal Reasoning &amp; CoT</div>
<ol class="bibliography">
    
<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/cot.jpg" class="teaser img-fluid z-depth-1">
    <abbr class="badge">arXiv25</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2501.13926">Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</a></div>
    <div class="author"><b class="publication-authors">Ziyu Guo*</b>, Renrui Zhang*, Chengzhuo Tong*, Zhizheng Zhao*, Peng Gao, Hongsheng Li#, Pheng-Ann Heng#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">Under Review</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2501.13926" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ZiyuGuo99/Image-Generation-CoT" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>


<br>
<br>

<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/t2i-r1.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">NeurIPS25</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2505.00703">T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT</a></div>
    <div class="author">Dongzhi Jiang*, <b>Ziyu Guo*</b>, Renrui Zhang*, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng#, Hongsheng Li#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">NeurIPS 2025</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2505.17017" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/CaraJ7/T2I-R1" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>

<br>
<br>

<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/delving.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">NeurIPS25</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2505.17017">Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO</a></div>
    <div class="author">Chengzhuo Tong*, <b>Ziyu Guo*</b>, Renrui Zhang*, Wenyu Shan*, Xinyu Wei, Zhenghao Xing, Hongsheng Li#, Pheng-Ann Heng#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">NeurIPS 2025</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2505.17017" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
    </div>
  </div>
</div>

<br>
<br>

<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/sciverse.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">ACL25</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2503.10627">SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems</a></div>
    <div class="author"><b>Ziyu Guo*</b>, Renrui Zhang*, Hao Chen*, Jialin Gao*, Dongzhi Jiang, Jiaze Wang, Pheng-Ann Heng#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">ACL 2025</i></strong>
    <div class="links">
      <a href="https://sciverse-cuhk.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Webpage</a>
      <a href="https://huggingface.co/datasets/ZiyuG/SciVerse" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Benchmark</a>
    </div>
  </div>
</div>

<br>
<br>


<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/mme-cot.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">ICML25</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2502.09621">MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency</a></div>
    <div class="author">Dongzhi Jiang*, Renrui Zhang*, <b class="publication-authors">Ziyu Guo</b>, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">ICML 2025</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2502.09621" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/CaraJ7/MME-CoT" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://mmecot.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Webpage</a>
      <a href="https://huggingface.co/datasets/CaraJ/MME-CoT" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Benchmark</a>
    </div>
  </div>
</div>

<br>
<br>

<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/teaser.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">ICLR25</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2305.03048.pdf">MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines</a></div>
    <div class="author">Dongzhi Jiang*, Renrui Zhang*, <b class="publication-authors">Ziyu Guo</b>, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song, Peng Gao, Yu Liu, Chunyuan Li, Hongsheng Li#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">ICLR 2025</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2409.12959" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/CaraJ7/MMSearch" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://mmsearch.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Webpage</a>
      <a href="https://huggingface.co/datasets/CaraJ/MMSearch" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Dataset</a>
    </div>
  </div>
</div>
</ol>

<div class="pub-section">ðŸ§© 3D Vision &amp; Representation Learning</div>

<ol class="bibliography">
<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/pointbind.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">arXiv23</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2309.00615">Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following</a></div>
    <div class="author"><b class="publication-authors">Ziyu Guo*</b>, Renrui Zhang*, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li#, Hongsheng Li, Pheng-Ann Heng.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">Under Review</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2309.00615" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ZiyuGuo99/Point-Bind_Point-LLM" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>


<br>
<br>
  
<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/stylemotif.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">ICCV25</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2503.21775">StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion</a></div>
    <div class="author"><b>Ziyu Guo</b>, Young Yoon Lee, Joseph Liu, Yizhak Ben-Shabat, Victor Zordan, Mubbasir Kapadia#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">ICCV 2025</i></strong>
    <div class="links">
      <a href="https://stylemotif.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Webpage</a>
    </div>
  </div>
</div>

<br>
<br>


<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/sam2point.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">arXiv24</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2408.16768">SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners</a></div>
    <div class="author"><b class="publication-authors">Ziyu Guo*</b>, Renrui Zhang*, Xiangyang Zhu*, Chengzhuo Tong, Peng Gao, Chunyuan Li, Pheng-Ann Heng#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">Technical Report</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2408.16768" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ZiyuGuo99/SAM2Point" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://sam2point.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Webpage</a>
      <a href="https://huggingface.co/spaces/ZiyuG/SAM2Point" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Demo</a>
    </div>
  </div>
</div>


<br>
<br>


<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/persam.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">ICLR24</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2305.03048.pdf">Personalize Segment Anything Model with One Shot</a></div>
    <div class="author">Renrui Zhang, Zhengkai Jiang*, <b class="publication-authors">Ziyu Guo*</b>, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, Hongsheng Li#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">ICLR 2024</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2305.03048.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ZrrSkywalker/Personalize-SAM" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>


<br>
<br>
    

<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/viewrefer.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">ICCV23</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2305.03048.pdf">ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance</a></div>
    <div class="author"><b class="publication-authors">Ziyu Guo*</b>, Yiwen Tang*, Renrui Zhang*, Dong Wang, Zhigang Wang, Bin Zhao.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">ICCV 2023</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2303.16894.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/Ivan-Tang-3D/ViewRefer3D" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>


<br>
<br>


<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/calip.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">AAAI23</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/25152/24924">CALIP: Zero-Shot Enhancement of CLIP with Non-Parametric Attention</a></div>
    <div class="author"><b class="publication-authors">Ziyu Guo*</b>, Renrui Zhang*, Longtian Qiu*, Xupeng Miao, Bin Cui#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">AAAI 2023 Oral</i></strong>
    <div class="links">
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25152/24924" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ZiyuGuo99/CALIP" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>



<br>
<br>



<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/jointmae.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">IJCAI23</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2302.14007">Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D 
Point Cloud Pre-training</a></div>
    <div class="author"><b class="publication-authors">Ziyu Guo*</b>, Renrui Zhang*, Longtian Qiu, Xianzhi Li#, Pheng-Ann Heng.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">IJCAI 2023 Oral</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2302.14007" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
    </div>
  </div>
</div>


<br>
<br>



<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/pointclip.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">CVPR22</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf">PointCLIP: Point Cloud Understanding by CLIP</a></div>
    <div class="author">Renrui Zhang*, <b class="publication-authors">Ziyu Guo*</b>, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, Hongsheng Li#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">CVPR 2022</i></strong>
    <div class="links">
      <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ZrrSkywalker/PointCLIP" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>


<br>
<br>


<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/pointm2ae.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">NeurIPS22</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2205.14401.pdf">Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training</a></div>
    <div class="author">Renrui Zhang, <b class="publication-authors">Ziyu Guo</b>, Rongyao Fang, Bin Zhao, Dong Wang, Yu Qiao, Hongsheng Li, Peng Gao#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">NeurIPS 2022</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2205.14401.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ZrrSkywalker/Point-M2AE" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>


<br>
<br>


<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/wacv.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">WACV23</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2303.00703.pdf">Nearest Neighbors Meet Deep Neural Networks for Point Cloud Analysis</a></div>
    <div class="author">Renrui Zhang, Liuhui Wang, <b class="publication-authors">Ziyu Guo</b>, Jianbo Shi#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">WACV 2023</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2303.00703.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
    </div>
  </div>
</div>


</div>
