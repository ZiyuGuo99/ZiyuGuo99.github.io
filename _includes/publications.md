<head>
  <style>
    .publication-title {
      color: blue;
    }
    .publication-authors {
      color: black;
    }
    .publication-periodical {
      color: purple;
    }
  </style>
</head>
<h2 id="publications" style="margin: 2px 0px -15px;">Selected Publications and Preprints</h2>

<div class="publications">
<ol class="bibliography">
  
<strong><i style="color:#7b5aa6">* Equal contribution, # Corresponding author</i></strong>

<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/sam2point.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">arXiv24</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2408.16768">SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners</a></div>
    <div class="author"><b class="publication-authors">Ziyu Guo</b>, Renrui Zhang, Xiangyang Zhu, Chengzhuo Tong, Peng Gao, Chunyuan Li, Pheng-Ann Heng.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">Technical Report</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2408.16768" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ZiyuGuo99/SAM2Point" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://sam2point.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Webpage</a>
      <a href="https://huggingface.co/spaces/ZiyuG/SAM2Point" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Demo</a>
    </div>
  </div>
</div>


<br>
<br>

<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/mmsearch.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">arXiv24</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2305.03048.pdf">MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines</a></div>
    <div class="author">Dongzhi Jiang*, Renrui Zhang*, <b class="publication-authors">Ziyu Guo</b>, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song, Peng Gao, Yu Liu, Chunyuan Li, Hongsheng Li.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">arXiv 2024</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2409.12959" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/CaraJ7/MMSearch" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://mmsearch.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Webpage</a>
      <a href="https://huggingface.co/datasets/CaraJ/MMSearch" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Dataset</a>
    </div>
  </div>
</div>


<br>
<br>

<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/pointbind.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">arXiv23</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2309.00615">Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following</a></div>
    <div class="author"><b class="publication-authors">Ziyu Guo</b>, Renrui Zhang#, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li#, Hongsheng Li, Pheng-Ann Heng.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">Under Review</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2309.00615" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ZiyuGuo99/Point-Bind_Point-LLM" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>


<br>
<br>
  

<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/persam.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">ICLR24</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2305.03048.pdf">Personalize Segment Anything Model with One Shot</a></div>
    <div class="author">Renrui Zhang, Zhengkai Jiang*, <b class="publication-authors">Ziyu Guo*</b>, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, Hongsheng Li#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">ICLR 2024</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2305.03048.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ZrrSkywalker/Personalize-SAM" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>


<br>
<br>
    

<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/viewrefer.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">ICCV23</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2305.03048.pdf">ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance</a></div>
    <div class="author"><b class="publication-authors">Ziyu Guo*</b>, Yiwen Tang*, Renrui Zhang*, Dong Wang, Zhigang Wang, Bin Zhao.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">ICCV 2023</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2303.16894.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/Ivan-Tang-3D/ViewRefer3D" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>


<br>
<br>


<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/calip.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">AAAI23</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/25152/24924">CALIP: Zero-Shot Enhancement of CLIP with Non-Parametric Attention</a></div>
    <div class="author"><b class="publication-authors">Ziyu Guo*</b>, Renrui Zhang*, Longtian Qiu*, Xupeng Miao, Bin Cui#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">AAAI 2023 Oral</i></strong>
    <div class="links">
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25152/24924" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ZiyuGuo99/CALIP" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>



<br>
<br>



<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/jointmae.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">IJCAI23</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2302.14007">Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D 
Point Cloud Pre-training</a></div>
    <div class="author"><b class="publication-authors">Ziyu Guo*</b>, Renrui Zhang*, Longtian Qiu, Xianzhi Li#, Pheng-Ann Heng.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">IJCAI 2023 Oral</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2302.14007" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
    </div>
  </div>
</div>


<br>
<br>



<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/pointclip.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">CVPR22</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf">PointCLIP: Point Cloud Understanding by CLIP</a></div>
    <div class="author">Renrui Zhang*, <b class="publication-authors">Ziyu Guo*</b>, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, Hongsheng Li#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">CVPR 2022</i></strong>
    <div class="links">
      <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ZrrSkywalker/PointCLIP" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>


<br>
<br>


<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/pointm2ae.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">NeurIPS22</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2205.14401.pdf">Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training</a></div>
    <div class="author">Renrui Zhang, <b class="publication-authors">Ziyu Guo</b>, Rongyao Fang, Bin Zhao, Dong Wang, Yu Qiao, Hongsheng Li, Peng Gao#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">NeurIPS 2022</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2205.14401.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ZrrSkywalker/Point-M2AE" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>


<br>
<br>


<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/wacv.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">WACV23</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/pdf/2303.00703.pdf">Nearest Neighbors Meet Deep Neural Networks for Point Cloud Analysis</a></div>
    <div class="author">Renrui Zhang, Liuhui Wang, <b class="publication-authors">Ziyu Guo</b>, Jianbo Shi#.</div>
    <div class="periodical"><strong><i style="color:#7b5aa6">WACV 2023</i></strong>
    <div class="links">
      <a href="https://arxiv.org/pdf/2303.00703.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
    </div>
  </div>
</div>


</div>
