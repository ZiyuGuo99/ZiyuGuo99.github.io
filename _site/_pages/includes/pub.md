# üìù Publications

<!-- <div style="background-color: #f0f7ff; border-left: 4px solid #0066cc; padding: 12px 16px; margin-bottom: 20px; border-radius: 4px;">
  <strong>üí°</strong> Click "Show Figure" to view the corresponding figure.
</div> -->

## CoT/CoF Reasoning for Visual Generation

<div class="paper-box">
  <div class="paper-box-text">
    <span class="preprint-tag">arXiv 2025</span> <a href="https://arxiv.org/abs/2511.16671">Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</a>.<br>
    <strong>Ziyu Guo</strong>*, Renrui Zhang‚Ä†*, Hongyu Li*, Manyuan Zhang‚Ä†, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, Pheng-Ann Heng.<br>[<a href="https://arxiv.org/abs/2511.16671">PDF</a>] [<a href="https://github.com/ZiyuGuo99/Thinking-while-Generating">Code</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-twig', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-twig" style="display: none;">
    <img src="images/papers/twig.png" alt="Thinking-while-Generating">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="report-tag">Technical Report</span> <a href="https://arxiv.org/abs/2510.26802">Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study on the MME-CoF Benchmark</a>.<br>
    <strong>Ziyu Guo</strong>*, Xinyan Chen*, Renrui Zhang*, Ruichuan An*, Yu Qi*, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng.<br>[<a href="https://arxiv.org/abs/2510.26802">PDF</a>] [<a href="https://github.com/ZiyuGuo99/MME-CoF">Code</a>] [<a href="https://video-cof.github.io/">Homepage</a>] [<a href="https://huggingface.co/datasets/ZiyuG/MME-CoF">Benchmark</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-1', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-1" style="display: none;">
    <img src="images/papers/video-cof.jpg" alt="MME-CoF">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="preprint-tag">arXiv 2025</span> <a href="https://arxiv.org/pdf/2501.13926">Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</a>.<br>
    <strong>Ziyu Guo</strong>*, Renrui Zhang*, Chengzhuo Tong*, Zhizheng Zhao*, Peng Gao, Hongsheng Li#, Pheng-Ann Heng#.<br>[<a href="https://arxiv.org/pdf/2501.13926">PDF</a>] [<a href="https://github.com/ZiyuGuo99/Image-Generation-CoT">Code</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-2', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-2" style="display: none;">
    <img src="images/papers/cot.jpg" alt="Image Generation CoT">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">NeurIPS 2025</span> <a href="https://arxiv.org/pdf/2505.00703">T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT</a>.<br>
    Dongzhi Jiang*, <strong>Ziyu Guo</strong>*, Renrui Zhang*, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng#, Hongsheng Li#.<br>[<a href="https://arxiv.org/pdf/2505.00703">PDF</a>] [<a href="https://github.com/CaraJ7/T2I-R1">Code</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-3', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-3" style="display: none;">
    <img src="images/papers/t2i-r1.png" alt="T2I-R1">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">NeurIPS 2025</span> <a href="https://arxiv.org/pdf/2505.17017">Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO</a>.<br>
    Chengzhuo Tong*, <strong>Ziyu Guo</strong>*, Renrui Zhang*, Wenyu Shan*, Xinyu Wei, Zhenghao Xing, Hongsheng Li#, Pheng-Ann Heng#.<br>[<a href="https://arxiv.org/pdf/2505.17017">PDF</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-4', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-4" style="display: none;">
    <img src="images/papers/delving.png" alt="Delving">
  </div>
</div>

## CoT Reasoning for Visual Understanding

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">ACL 2025</span> <a href="https://arxiv.org/pdf/2503.10627">SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems</a>.<br>
    <strong>Ziyu Guo</strong>*, Renrui Zhang*, Hao Chen*, Jialin Gao*, Dongzhi Jiang, Jiaze Wang, Pheng-Ann Heng#.<br>[<a href="https://sciverse-cuhk.github.io/">Webpage</a>] [<a href="https://huggingface.co/datasets/ZiyuG/SciVerse">Benchmark</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-5', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-5" style="display: none;">
    <img src="images/papers/sciverse.png" alt="SciVerse">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">ICML 2025</span> <a href="https://arxiv.org/pdf/2502.09621">MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency</a>.<br>
    Dongzhi Jiang*, Renrui Zhang*, <strong>Ziyu Guo</strong>, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li#.<br>[<a href="https://arxiv.org/pdf/2502.09621">PDF</a>] [<a href="https://github.com/CaraJ7/MME-CoT">Code</a>] [<a href="https://mmecot.github.io/">Webpage</a>] [<a href="https://huggingface.co/datasets/CaraJ/MME-CoT">Benchmark</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-6', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-6" style="display: none;">
    <img src="images/papers/mme-cot.png" alt="MME-CoT">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">ICLR 2025</span> <a href="https://arxiv.org/pdf/2407.08739">MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine</a>.<br>
    Renrui Zhang, Xinyu Wei, Dongzhi Jiang, <strong>Ziyu Guo</strong>, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, Peng Gao, Chunyuan Li, Hongsheng Li#.<br>[<a href="https://arxiv.org/pdf/2407.08739">PDF</a>] [<a href="https://github.com/ZrrSkywalker/MAVIS">Code</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-7', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-7" style="display: none;">
    <img src="images/papers/mavis.png" alt="MAVIS">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">ECCV 2024</span> <a href="https://arxiv.org/pdf/2403.14624">MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</a>.<br>
    Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, <strong>Ziyu Guo</strong>, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, Peng Gao, Hongsheng Li.<br>[<a href="https://mathverse-cuhk.github.io/">Webpage</a>] [<a href="https://huggingface.co/datasets/AI4Math/MathVerse">Benchmark</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-8', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-8" style="display: none;">
    <img src="images/papers/mathverse.png" alt="MathVerse">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">ICLR 2025</span> <a href="https://arxiv.org/pdf/2409.12959">MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines</a>.<br>
    Dongzhi Jiang*, Renrui Zhang*, <strong>Ziyu Guo</strong>, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song, Peng Gao, Yu Liu, Chunyuan Li, Hongsheng Li#.<br>[<a href="https://arxiv.org/pdf/2409.12959">PDF</a>] [<a href="https://github.com/CaraJ7/MMSearch">Code</a>] [<a href="https://mmsearch.github.io/">Webpage</a>] [<a href="https://huggingface.co/datasets/CaraJ/MMSearch">Dataset</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-9', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-9" style="display: none;">
    <img src="images/papers/teaser.png" alt="MMSearch">
  </div>
</div>

## 3D Large Models

<div class="paper-box">
  <div class="paper-box-text">
    <span class="preprint-tag">arXiv 2023</span> <a href="https://arxiv.org/pdf/2309.00615">Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following</a>.<br>
    <strong>Ziyu Guo</strong>*, Renrui Zhang*, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li#, Hongsheng Li, Pheng-Ann Heng.<br>[<a href="https://arxiv.org/pdf/2309.00615">PDF</a>] [<a href="https://github.com/ZiyuGuo99/Point-Bind_Point-LLM">Code</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-10', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-10" style="display: none;">
    <img src="images/papers/pointbind.png" alt="Point-Bind">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="preprint-tag">arXiv 2025</span> <a href="https://arxiv.org/abs/2502.09620">Exploring the Potential of Encoder-free Architectures in 3D LMMs</a>.<br>
    Yiwen Tang*, <strong>Ziyu Guo</strong>*, Zhuhao Wang*, Renrui Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao.<br>[<a href="https://arxiv.org/abs/2502.09620">PDF</a>] [<a href="https://github.com/Ivan-Tang-3D/ENEL">Code</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-11', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-11" style="display: none;">
    <img src="images/papers/enel.png" alt="Encoder-free">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="report-tag">Technical Report</span> <a href="https://arxiv.org/pdf/2408.16768">SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners</a>.<br>
    <strong>Ziyu Guo</strong>*, Renrui Zhang*, Xiangyang Zhu*, Chengzhuo Tong, Peng Gao, Chunyuan Li, Pheng-Ann Heng#.<br>[<a href="https://arxiv.org/pdf/2408.16768">PDF</a>] [<a href="https://github.com/ZiyuGuo99/SAM2Point">Code</a>] [<a href="https://sam2point.github.io/">Webpage</a>] [<a href="https://huggingface.co/spaces/ZiyuG/SAM2Point">Demo</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-12', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-12" style="display: none;">
    <img src="images/papers/sam2point.png" alt="SAM2Point">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">CVPR 2022</span> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf">PointCLIP: Point Cloud Understanding by CLIP</a>.<br>
    Renrui Zhang*, <strong>Ziyu Guo</strong>*, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, Hongsheng Li#.<br>[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf">PDF</a>] [<a href="https://github.com/ZrrSkywalker/PointCLIP">Code</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-13', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-13" style="display: none;">
    <img src="images/papers/pointclip.png" alt="PointCLIP">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">NeurIPS 2022</span> <a href="https://arxiv.org/pdf/2205.14401.pdf">Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training</a>.<br>
    Renrui Zhang, <strong>Ziyu Guo</strong>, Rongyao Fang, Bin Zhao, Dong Wang, Yu Qiao, Hongsheng Li, Peng Gao#.<br>[<a href="https://arxiv.org/pdf/2205.14401.pdf">PDF</a>] [<a href="https://github.com/ZrrSkywalker/Point-M2AE">Code</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-14', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-14" style="display: none;">
    <img src="images/papers/pointm2ae.png" alt="Point-M2AE">
  </div>
</div>

## 3D & Multi-modality Learning

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">ICCV 2025</span> <a href="https://arxiv.org/pdf/2503.21775">StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion</a>.<br>
    <strong>Ziyu Guo</strong>, Young Yoon Lee, Joseph Liu, Yizhak Ben-Shabat, Victor Zordan, Mubbasir Kapadia#.<br>[<a href="https://stylemotif.github.io/">Webpage</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-15', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-15" style="display: none;">
    <img src="images/papers/stylemotif.png" alt="StyleMotif">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">ICLR 2024</span> <a href="https://arxiv.org/pdf/2305.03048.pdf">Personalize Segment Anything Model with One Shot</a>.<br>
    Renrui Zhang, Zhengkai Jiang*, <strong>Ziyu Guo</strong>*, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, Hongsheng Li#.<br>[<a href="https://arxiv.org/pdf/2305.03048.pdf">PDF</a>] [<a href="https://github.com/ZrrSkywalker/Personalize-SAM">Code</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-16', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-16" style="display: none;">
    <img src="images/papers/persam.png" alt="PerSAM">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">ICCV 2023</span> <a href="https://arxiv.org/pdf/2303.16894.pdf">ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance</a>.<br>
    <strong>Ziyu Guo</strong>*, Yiwen Tang*, Renrui Zhang*, Dong Wang, Zhigang Wang, Bin Zhao.<br>[<a href="https://arxiv.org/pdf/2303.16894.pdf">PDF</a>] [<a href="https://github.com/Ivan-Tang-3D/ViewRefer3D">Code</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-17', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-17" style="display: none;">
    <img src="images/papers/viewrefer.png" alt="ViewRefer">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">AAAI 2023</span> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25152/24924">CALIP: Zero-Shot Enhancement of CLIP with Non-Parametric Attention</a>.<br>
    <strong>Ziyu Guo</strong>*, Renrui Zhang*, Longtian Qiu*, Xupeng Miao, Bin Cui#.<br>[<a href="https://ojs.aaai.org/index.php/AAAI/article/view/25152/24924">PDF</a>] [<a href="https://github.com/ZiyuGuo99/CALIP">Code</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-18', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-18" style="display: none;">
    <img src="images/papers/calip.png" alt="CALIP">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">IJCAI 2023</span> <a href="https://arxiv.org/pdf/2302.14007">Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training</a>.<br>
    <strong>Ziyu Guo</strong>*, Renrui Zhang*, Longtian Qiu, Xianzhi Li#, Pheng-Ann Heng.<br>[<a href="https://arxiv.org/pdf/2302.14007">PDF</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-19', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-19" style="display: none;">
    <img src="images/papers/jointmae.png" alt="Joint-MAE">
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-text">
    <span class="conference-tag">WACV 2023</span> <a href="https://arxiv.org/pdf/2303.00703.pdf">Nearest Neighbors Meet Deep Neural Networks for Point Cloud Analysis</a>.<br>
    Renrui Zhang, Liuhui Wang, <strong>Ziyu Guo</strong>, Jianbo Shi#.<br>[<a href="https://arxiv.org/pdf/2303.00703.pdf">PDF</a>] <button class="show-figure-btn" onclick="toggleFigure('fig-20', this)">Show Figure</button>
  </div>
  <div class="paper-box-image-below" id="fig-20" style="display: none;">
    <img src="images/papers/wacv.png" alt="WACV">
  </div>
</div>

<script>
function toggleFigure(figId, button) {
  var figure = document.getElementById(figId);
  
  if (figure.style.display === 'none' || figure.style.display === '') {
    figure.style.display = 'block';
    button.innerHTML = 'Hide Figure';
    button.classList.add('active');
  } else {
    figure.style.display = 'none';
    button.innerHTML = 'Show Figure';
    button.classList.remove('active');
  }
}
</script>
